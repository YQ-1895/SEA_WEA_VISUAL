{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0706e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Processing CSV1 (All reservoirs data)...\n",
      "ðŸ“Š CSV1 Original records: 369\n",
      "ðŸ“Š CSV1 After status filtering: 369\n",
      "ðŸ“Š Status distribution: {'Operational': 369}\n",
      "ðŸ”„ Processing CSV2 (Critical reservoirs data)...\n",
      "ðŸ“Š CSV2 Records: 360\n",
      "âœ… JSON1 saved to ../processed_all_reservoirs.json\n",
      "âœ… JSON2 saved to ../processed_critical_reservoirs.json\n",
      "ðŸ“Š Processing Summary:\n",
      "  Total reservoirs (filtered): 369\n",
      "  Current (Operational): 369\n",
      "  Future (Planned + Under construction): 0\n",
      "  Critical reservoirs: 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15330\\AppData\\Local\\Temp\\ipykernel_6468\\2558357492.py:83: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['is_critical_reservoir'] = df['max_capacity_mcm'].fillna(0) >= 100\n",
      "C:\\Users\\15330\\AppData\\Local\\Temp\\ipykernel_6468\\2558357492.py:85: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.loc[df['normal_capacity_mcm'].fillna(0) >= 100, 'is_critical_reservoir'] = True\n",
      "C:\\Users\\15330\\AppData\\Local\\Temp\\ipykernel_6468\\2558357492.py:87: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.loc[df['min_capacity_mcm'].fillna(0) >= 100, 'is_critical_reservoir'] = True\n",
      "C:\\Users\\15330\\AppData\\Local\\Temp\\ipykernel_6468\\2558357492.py:91: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['is_critical_hydropower'] = df['power_mw'].fillna(0) >= 30\n",
      "C:\\Users\\15330\\AppData\\Local\\Temp\\ipykernel_6468\\2558357492.py:83: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['is_critical_reservoir'] = df['max_capacity_mcm'].fillna(0) >= 100\n",
      "C:\\Users\\15330\\AppData\\Local\\Temp\\ipykernel_6468\\2558357492.py:85: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.loc[df['normal_capacity_mcm'].fillna(0) >= 100, 'is_critical_reservoir'] = True\n",
      "C:\\Users\\15330\\AppData\\Local\\Temp\\ipykernel_6468\\2558357492.py:87: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.loc[df['min_capacity_mcm'].fillna(0) >= 100, 'is_critical_reservoir'] = True\n",
      "C:\\Users\\15330\\AppData\\Local\\Temp\\ipykernel_6468\\2558357492.py:91: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['is_critical_hydropower'] = df['power_mw'].fillna(0) >= 30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def process_seawea_data(csv1_path, csv2_path, output_json1, output_json2):\n",
    "    \"\"\"\n",
    "    Process CSV1 (all reservoirs) and CSV2 (critical reservoirs) for the SEA-WEA dashboard\n",
    "    \n",
    "    Args:\n",
    "        csv1_path: Path to 13-normalize_column_type_fixed_add_SEAWEA_ID.csv\n",
    "        csv2_path: Path to 16_ciritical_reservoir_with_polygon_20250812_add_SEAWEA_ID_add_INDEX.csv\n",
    "        output_json1: Output path for processed all reservoirs JSON\n",
    "        output_json2: Output path for processed critical reservoirs JSON\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Processing CSV1 (All reservoirs data)...\")\n",
    "    \n",
    "    # Load CSV1 - all reservoirs\n",
    "    df1 = pd.read_csv(csv1_path)\n",
    "    \n",
    "    # Remove columns to the right of dynamic_id as specified\n",
    "    if 'dynamic_id' in df1.columns:\n",
    "        dynamic_id_index = df1.columns.get_loc('dynamic_id')\n",
    "        df1 = df1.iloc[:, :dynamic_id_index + 1]\n",
    "    \n",
    "    # Filter for required statuses only: Operational, Planned, Under_construction\n",
    "    valid_statuses = ['Operational', 'Planned', 'Under_construction']\n",
    "    df1_filtered = df1[df1['status'].isin(valid_statuses)].copy()\n",
    "    \n",
    "    print(f\"ðŸ“Š CSV1 Original records: {len(df1)}\")\n",
    "    print(f\"ðŸ“Š CSV1 After status filtering: {len(df1_filtered)}\")\n",
    "    print(f\"ðŸ“Š Status distribution: {df1_filtered['status'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Process CSV1\n",
    "    json1_data = process_dataframe_to_json(df1_filtered, \"all_reservoirs\")\n",
    "    \n",
    "    print(\"ðŸ”„ Processing CSV2 (Critical reservoirs data)...\")\n",
    "    \n",
    "    # Load CSV2 - critical reservoirs\n",
    "    df2 = pd.read_csv(csv2_path)\n",
    "    \n",
    "    print(f\"ðŸ“Š CSV2 Records: {len(df2)}\")\n",
    "    \n",
    "    # Process CSV2\n",
    "    json2_data = process_dataframe_to_json(df2, \"critical_reservoirs\")\n",
    "    \n",
    "    # Save JSON files\n",
    "    save_json_safely(json1_data, output_json1)\n",
    "    save_json_safely(json2_data, output_json2)\n",
    "    \n",
    "    print(f\"âœ… JSON1 saved to {output_json1}\")\n",
    "    print(f\"âœ… JSON2 saved to {output_json2}\")\n",
    "    \n",
    "    # Return summary information\n",
    "    return {\n",
    "        'csv1_total': len(df1_filtered),\n",
    "        'csv1_operational': len(df1_filtered[df1_filtered['status'] == 'Operational']),\n",
    "        'csv1_future': len(df1_filtered[df1_filtered['status'].isin(['Planned', 'Under_construction'])]),\n",
    "        'csv2_total': len(df2)\n",
    "    }\n",
    "\n",
    "def process_dataframe_to_json(df, dataset_type):\n",
    "    \"\"\"\n",
    "    Convert DataFrame to JSON format suitable for the dashboard\n",
    "    \"\"\"\n",
    "    # Replace invalid values with None\n",
    "    df = df.replace(-999.9, None)\n",
    "    df = df.replace(-99, None)\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    \n",
    "    # Clean and validate coordinates\n",
    "    df = df.dropna(subset=['longitude', 'latitude'])\n",
    "    df = df[(df['longitude'] >= -180) & (df['longitude'] <= 180)]\n",
    "    df = df[(df['latitude'] >= -90) & (df['latitude'] <= 90)]\n",
    "    \n",
    "    # Add classification flags for visualization\n",
    "    df['is_critical_reservoir'] = False\n",
    "    df['is_critical_hydropower'] = False\n",
    "    \n",
    "    # For reservoirs: critical if capacity >= 100 MCM\n",
    "    if 'max_capacity_mcm' in df.columns:\n",
    "        df['is_critical_reservoir'] = df['max_capacity_mcm'].fillna(0) >= 100\n",
    "    if 'normal_capacity_mcm' in df.columns:\n",
    "        df.loc[df['normal_capacity_mcm'].fillna(0) >= 100, 'is_critical_reservoir'] = True\n",
    "    if 'min_capacity_mcm' in df.columns:\n",
    "        df.loc[df['min_capacity_mcm'].fillna(0) >= 100, 'is_critical_reservoir'] = True\n",
    "    \n",
    "    # For hydropower: critical if power >= 30 MW\n",
    "    if 'power_mw' in df.columns:\n",
    "        df['is_critical_hydropower'] = df['power_mw'].fillna(0) >= 30\n",
    "    \n",
    "    # Add visualization categories\n",
    "    df['capacity_category'] = df.apply(lambda row: get_capacity_category(row), axis=1)\n",
    "    df['power_category'] = df.apply(lambda row: get_power_category(row), axis=1)\n",
    "    \n",
    "    # Create summary statistics\n",
    "    summary = create_summary_stats(df, dataset_type)\n",
    "    \n",
    "    # Prepare final dataset structure\n",
    "    processed_data = {\n",
    "        'features': df.to_dict('records'),\n",
    "        'summary': summary,\n",
    "        'metadata': {\n",
    "            'total_records': len(df),\n",
    "            'dataset_type': dataset_type,\n",
    "            'last_updated': pd.Timestamp.now().isoformat(),\n",
    "            'filters': {\n",
    "                'countries': sorted(df['country'].dropna().unique().tolist()) if 'country' in df.columns else [],\n",
    "                'statuses': sorted(df['status'].dropna().unique().tolist()) if 'status' in df.columns else [],\n",
    "                'main_uses': sorted(df['main_use'].dropna().unique().tolist()) if 'main_use' in df.columns else [],\n",
    "                'basins': sorted(df['basin'].dropna().unique().tolist()) if 'basin' in df.columns else []\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def get_capacity_category(row):\n",
    "    \"\"\"Determine capacity category for visualization\"\"\"\n",
    "    # Check multiple capacity columns\n",
    "    capacity_cols = ['max_capacity_mcm', 'normal_capacity_mcm', 'min_capacity_mcm']\n",
    "    max_capacity = 0\n",
    "    \n",
    "    for col in capacity_cols:\n",
    "        if col in row and pd.notnull(row[col]) and row[col] > 0:\n",
    "            max_capacity = max(max_capacity, row[col])\n",
    "    \n",
    "    if max_capacity <= 0:\n",
    "        return 'Unknown'\n",
    "    elif max_capacity <= 10:\n",
    "        return '0-10MCM'\n",
    "    elif max_capacity <= 100:\n",
    "        return '10-100MCM'\n",
    "    elif max_capacity <= 1000:\n",
    "        return '100-1000MCM'\n",
    "    else:\n",
    "        return '>1000MCM'\n",
    "\n",
    "def get_power_category(row):\n",
    "    \"\"\"Determine power category for visualization\"\"\"\n",
    "    power = row.get('power_mw', 0)\n",
    "    \n",
    "    if pd.isna(power) or power <= 0:\n",
    "        return 'Unknown'\n",
    "    elif power <= 30:\n",
    "        return '0-30MW'\n",
    "    elif power <= 100:\n",
    "        return '30-100MW'\n",
    "    elif power <= 1000:\n",
    "        return '100-1000MW'\n",
    "    else:\n",
    "        return '>1000MW'\n",
    "\n",
    "def create_summary_stats(df, dataset_type):\n",
    "    \"\"\"Create summary statistics for the dataset\"\"\"\n",
    "    summary = {\n",
    "        'total_count': len(df),\n",
    "        'dataset_type': dataset_type\n",
    "    }\n",
    "    \n",
    "    # Add country statistics\n",
    "    if 'country' in df.columns:\n",
    "        summary['countries'] = df['country'].value_counts().to_dict()\n",
    "    \n",
    "    # Add status statistics\n",
    "    if 'status' in df.columns:\n",
    "        summary['statuses'] = df['status'].value_counts().to_dict()\n",
    "        summary['current_count'] = len(df[df['status'] == 'Operational'])\n",
    "        summary['future_count'] = len(df[df['status'].isin(['Planned', 'Under_construction'])])\n",
    "    \n",
    "    # Add main use statistics\n",
    "    if 'main_use' in df.columns:\n",
    "        summary['main_uses'] = df['main_use'].value_counts().to_dict()\n",
    "    \n",
    "    # Add capacity and power distributions\n",
    "    summary['capacity_distribution'] = df['capacity_category'].value_counts().to_dict()\n",
    "    summary['power_distribution'] = df['power_category'].value_counts().to_dict()\n",
    "    \n",
    "    # Count critical facilities\n",
    "    summary['critical_reservoirs'] = int(df['is_critical_reservoir'].sum())\n",
    "    summary['critical_hydropower'] = int(df['is_critical_hydropower'].sum())\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def save_json_safely(data, output_file):\n",
    "    \"\"\"Save data to JSON with proper NaN handling\"\"\"\n",
    "    def clean_nans(obj):\n",
    "        \"\"\"Recursively convert all NaN, inf, -inf to None for JSON safety\"\"\"\n",
    "        if isinstance(obj, float):\n",
    "            if math.isnan(obj) or math.isinf(obj):\n",
    "                return None\n",
    "            return obj\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: clean_nans(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [clean_nans(v) for v in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    # Clean processed_data recursively\n",
    "    safe_data = clean_nans(data)\n",
    "    \n",
    "    # Save with proper formatting\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(safe_data, f, indent=2, allow_nan=False, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    summary = process_seawea_data(\n",
    "        csv1_path='13-allData_addSEAWEA_ID.csv',\n",
    "        # csv1_path='14-reservoirs_30MW_and_100MCM.csv',\n",
    "        csv2_path='16_ciriticalReservoir_addSEAWEA_ID_addINDEX.csv',\n",
    "        output_json1='../processed_all_reservoirs.json',\n",
    "        output_json2='../processed_critical_reservoirs.json'\n",
    "    )\n",
    " \n",
    "    print(\"ðŸ“Š Processing Summary:\")\n",
    "    print(f\"  Total reservoirs (filtered): {summary['csv1_total']}\")\n",
    "    print(f\"  Current (Operational): {summary['csv1_operational']}\")\n",
    "    print(f\"  Future (Planned + Under construction): {summary['csv1_future']}\")\n",
    "    print(f\"  Critical reservoirs: {summary['csv2_total']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64c63bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Processing CSV1 (All reservoirs data)...\n",
      "ðŸ“Š CSV1 Original records: 2075\n",
      "ðŸ“Š CSV1 After status filtering: 1980\n",
      "ðŸ“Š Status distribution: {'Operational': 1685, 'Planned': 295}\n",
      "ðŸ”„ Processing CSV2 (Critical reservoirs data)...\n",
      "ðŸ“Š CSV2 Records: 360\n",
      "âœ… JSON1 saved to ../processed_all_reservoirs.json\n",
      "âœ… JSON2 saved to ../processed_critical_reservoirs.json\n",
      "ðŸ“Š Processing Summary:\n",
      "  Total reservoirs (filtered): 1980\n",
      "  Current (Operational): 1685\n",
      "  Future (Planned + Under construction): 295\n",
      "  Critical reservoirs: 360\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def process_seawea_data(csv1_path, csv2_path, output_json1, output_json2):\n",
    "    \"\"\"\n",
    "    Process CSV1 (all reservoirs) and CSV2 (critical reservoirs) for the SEA-WEA dashboard\n",
    "    \n",
    "    Args:\n",
    "        csv1_path: Path to 13-normalize_column_type_fixed_add_SEAWEA_ID.csv\n",
    "        csv2_path: Path to 16_ciritical_reservoir_with_polygon_20250812_add_SEAWEA_ID_add_INDEX.csv\n",
    "        output_json1: Output path for processed all reservoirs JSON\n",
    "        output_json2: Output path for processed critical reservoirs JSON\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Processing CSV1 (All reservoirs data)...\")\n",
    "\n",
    "    # Record which cols should be checked through and should be numerical\n",
    "    num_cols = [\n",
    "        \"longitude\", \"latitude\",\n",
    "        \"max_capacity_mcm\", \"normal_capacity_mcm\", \"min_capacity_mcm\",\n",
    "        \"power_mw\"\n",
    "    ]\n",
    "    \n",
    "    # Load CSV1 - all reservoirs\n",
    "    df1 = pd.read_csv(csv1_path)\n",
    "\n",
    "    for col in num_cols:\n",
    "        if col in df1.columns:\n",
    "            # åŽ»åƒåˆ†ä½é€—å·ã€åŽ»å•ä½ä¸Žéžæ•°å­—å­—ç¬¦ï¼Œå†è½¬æ•°å€¼\n",
    "            s = df1[col].astype(str).str.strip().str.replace(\",\", \"\", regex=False)\n",
    "            s = s.str.replace(r\"[^0-9eE+\\-\\.]\", \"\", regex=True)\n",
    "            df1[col] = pd.to_numeric(s, errors=\"coerce\")\n",
    "    \n",
    "    # Remove columns to the right of dynamic_id as specified\n",
    "    if 'dynamic_id' in df1.columns:\n",
    "        dynamic_id_index = df1.columns.get_loc('dynamic_id')\n",
    "        df1 = df1.iloc[:, :dynamic_id_index + 1]\n",
    "    \n",
    "    # Filter for required statuses only: Operational, Planned, Under_construction\n",
    "    valid_statuses = ['Operational', 'Planned', 'Under_construction']\n",
    "    df1_filtered = df1[df1['status'].isin(valid_statuses)].copy()\n",
    "    \n",
    "    print(f\"ðŸ“Š CSV1 Original records: {len(df1)}\")\n",
    "    print(f\"ðŸ“Š CSV1 After status filtering: {len(df1_filtered)}\")\n",
    "    print(f\"ðŸ“Š Status distribution: {df1_filtered['status'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Process CSV1\n",
    "    json1_data = process_dataframe_to_json(df1_filtered, \"all_reservoirs\")\n",
    "    \n",
    "    print(\"ðŸ”„ Processing CSV2 (Critical reservoirs data)...\")\n",
    "    \n",
    "    # Load CSV2 - critical reservoirs\n",
    "    df2 = pd.read_csv(csv2_path)\n",
    "    \n",
    "    print(f\"ðŸ“Š CSV2 Records: {len(df2)}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    for col in num_cols:\n",
    "        if col in df2.columns:\n",
    "            s = df2[col].astype(str).str.strip().str.replace(\",\", \"\", regex=False)\n",
    "            s = s.str.replace(r\"[^0-9eE+\\-\\.]\", \"\", regex=True)\n",
    "            df2[col] = pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    \n",
    "    # Process CSV2\n",
    "    json2_data = process_dataframe_to_json(df2, \"critical_reservoirs\")\n",
    "    \n",
    "    # Save JSON files\n",
    "    save_json_safely(json1_data, output_json1)\n",
    "    save_json_safely(json2_data, output_json2)\n",
    "    \n",
    "    print(f\"âœ… JSON1 saved to {output_json1}\")\n",
    "    print(f\"âœ… JSON2 saved to {output_json2}\")\n",
    "    \n",
    "    # Return summary information\n",
    "    return {\n",
    "        'csv1_total': len(df1_filtered),\n",
    "        'csv1_operational': len(df1_filtered[df1_filtered['status'] == 'Operational']),\n",
    "        'csv1_future': len(df1_filtered[df1_filtered['status'].isin(['Planned', 'Under_construction'])]),\n",
    "        'csv2_total': len(df2)\n",
    "    }\n",
    "\n",
    "def process_dataframe_to_json(df, dataset_type):\n",
    "    \"\"\"\n",
    "    Convert DataFrame to JSON format suitable for the dashboard\n",
    "    \"\"\"\n",
    "    # Replace invalid values with None\n",
    "    df = df.replace(-999.9, None)\n",
    "    df = df.replace(-99, None)\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    \n",
    "    # Clean and validate coordinates\n",
    "    df = df.dropna(subset=['longitude', 'latitude'])\n",
    "    df = df[(df['longitude'] >= -180) & (df['longitude'] <= 180)]\n",
    "    df = df[(df['latitude'] >= -90) & (df['latitude'] <= 90)]\n",
    "    \n",
    "    # Add classification flags for visualization\n",
    "    df['is_critical_reservoir'] = False\n",
    "    df['is_critical_hydropower'] = False\n",
    "    \n",
    "    # For reservoirs: critical if capacity >= 100 MCM\n",
    "    if 'max_capacity_mcm' in df.columns:\n",
    "        df['is_critical_reservoir'] = df['max_capacity_mcm'].fillna(0) >= 100\n",
    "    if 'normal_capacity_mcm' in df.columns:\n",
    "        df.loc[df['normal_capacity_mcm'].fillna(0) >= 100, 'is_critical_reservoir'] = True\n",
    "    if 'min_capacity_mcm' in df.columns:\n",
    "        df.loc[df['min_capacity_mcm'].fillna(0) >= 100, 'is_critical_reservoir'] = True\n",
    "    \n",
    "    # For hydropower: critical if power >= 30 MW\n",
    "    if 'power_mw' in df.columns:\n",
    "        df['is_critical_hydropower'] = df['power_mw'].fillna(0) >= 30\n",
    "    \n",
    "    # Add visualization categories\n",
    "    df['capacity_category'] = df.apply(lambda row: get_capacity_category(row), axis=1)\n",
    "    df['power_category'] = df.apply(lambda row: get_power_category(row), axis=1)\n",
    "    \n",
    "    # Create summary statistics\n",
    "    summary = create_summary_stats(df, dataset_type)\n",
    "    \n",
    "    # Prepare final dataset structure\n",
    "    processed_data = {\n",
    "        'features': df.to_dict('records'),\n",
    "        'summary': summary,\n",
    "        'metadata': {\n",
    "            'total_records': len(df),\n",
    "            'dataset_type': dataset_type,\n",
    "            'last_updated': pd.Timestamp.now().isoformat(),\n",
    "            'filters': {\n",
    "                'countries': sorted(df['country'].dropna().unique().tolist()) if 'country' in df.columns else [],\n",
    "                'statuses': sorted(df['status'].dropna().unique().tolist()) if 'status' in df.columns else [],\n",
    "                'main_uses': sorted(df['main_use'].dropna().unique().tolist()) if 'main_use' in df.columns else [],\n",
    "                'basins': sorted(df['basin'].dropna().unique().tolist()) if 'basin' in df.columns else []\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def get_capacity_category(row):\n",
    "    \"\"\"Determine capacity category for visualization\"\"\"\n",
    "    # Check multiple capacity columns\n",
    "    capacity_cols = ['max_capacity_mcm', 'normal_capacity_mcm', 'min_capacity_mcm']\n",
    "    max_capacity = 0\n",
    "    \n",
    "    for col in capacity_cols:\n",
    "        if col in row and pd.notnull(row[col]) and row[col] > 0:\n",
    "            max_capacity = max(max_capacity, row[col])\n",
    "    \n",
    "    if max_capacity <= 0:\n",
    "        return 'Unknown'\n",
    "    elif max_capacity <= 10:\n",
    "        return '0-10MCM'\n",
    "    elif max_capacity <= 100:\n",
    "        return '10-100MCM'\n",
    "    elif max_capacity <= 1000:\n",
    "        return '100-1000MCM'\n",
    "    else:\n",
    "        return '>1000MCM'\n",
    "\n",
    "def get_power_category(row):\n",
    "    \"\"\"Determine power category for visualization\"\"\"\n",
    "    power = row.get('power_mw', 0)\n",
    "    \n",
    "    if pd.isna(power) or power <= 0:\n",
    "        return 'Unknown'\n",
    "    elif power <= 30:\n",
    "        return '0-30MW'\n",
    "    elif power <= 100:\n",
    "        return '30-100MW'\n",
    "    elif power <= 1000:\n",
    "        return '100-1000MW'\n",
    "    else:\n",
    "        return '>1000MW'\n",
    "\n",
    "def create_summary_stats(df, dataset_type):\n",
    "    \"\"\"Create summary statistics for the dataset\"\"\"\n",
    "    summary = {\n",
    "        'total_count': len(df),\n",
    "        'dataset_type': dataset_type\n",
    "    }\n",
    "    \n",
    "    # Add country statistics\n",
    "    if 'country' in df.columns:\n",
    "        summary['countries'] = df['country'].value_counts().to_dict()\n",
    "    \n",
    "    # Add status statistics\n",
    "    if 'status' in df.columns:\n",
    "        summary['statuses'] = df['status'].value_counts().to_dict()\n",
    "        summary['current_count'] = len(df[df['status'] == 'Operational'])\n",
    "        summary['future_count'] = len(df[df['status'].isin(['Planned', 'Under_construction'])])\n",
    "    \n",
    "    # Add main use statistics\n",
    "    if 'main_use' in df.columns:\n",
    "        summary['main_uses'] = df['main_use'].value_counts().to_dict()\n",
    "    \n",
    "    # Add capacity and power distributions\n",
    "    summary['capacity_distribution'] = df['capacity_category'].value_counts().to_dict()\n",
    "    summary['power_distribution'] = df['power_category'].value_counts().to_dict()\n",
    "    \n",
    "    # Count critical facilities\n",
    "    summary['critical_reservoirs'] = int(df['is_critical_reservoir'].sum())\n",
    "    summary['critical_hydropower'] = int(df['is_critical_hydropower'].sum())\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def save_json_safely(data, output_file):\n",
    "    \"\"\"Save data to JSON with proper NaN handling\"\"\"\n",
    "    def clean_nans(obj):\n",
    "        \"\"\"Recursively convert all NaN, inf, -inf to None for JSON safety\"\"\"\n",
    "        if isinstance(obj, float):\n",
    "            if math.isnan(obj) or math.isinf(obj):\n",
    "                return None\n",
    "            return obj\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: clean_nans(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [clean_nans(v) for v in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    # Clean processed_data recursively\n",
    "    safe_data = clean_nans(data)\n",
    "    \n",
    "    # Save with proper formatting\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(safe_data, f, indent=2, allow_nan=False, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    summary = process_seawea_data(\n",
    "        csv1_path='13-allData_addSEAWEA_ID-filtered.csv',\n",
    "        # csv1_path='14-reservoirs_30MW_and_100MCM.csv',\n",
    "        csv2_path='16_ciriticalReservoir_addSEAWEA_ID_addINDEX.csv',\n",
    "        output_json1='../processed_all_reservoirs.json',\n",
    "        output_json2='../processed_critical_reservoirs.json'\n",
    "    )\n",
    " \n",
    "    print(\"ðŸ“Š Processing Summary:\")\n",
    "    print(f\"  Total reservoirs (filtered): {summary['csv1_total']}\")\n",
    "    print(f\"  Current (Operational): {summary['csv1_operational']}\")\n",
    "    print(f\"  Future (Planned + Under construction): {summary['csv1_future']}\")\n",
    "    print(f\"  Critical reservoirs: {summary['csv2_total']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
